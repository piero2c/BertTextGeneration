\documentclass{article}

\usepackage[utf8]{inputenc}

\usepackage[margin=1in]{geometry}
\usepackage[titletoc,title]{appendix}
\usepackage{booktabs}
\usepackage{amsmath,amsfonts,amssymb,mathtools}

\usepackage{graphicx,float}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algorithmic}
\usepackage{tikz}
\usepackage[portuguese]{babel}
\usepackage{booktabs}

% ref packages
\usepackage{nameref}
% folowing  must be in this order
\usepackage{varioref}
\usepackage{hyperref}
\usepackage{cleveref}

\usepackage{tabularx}
\newcolumntype{L}{>{\raggedright\arraybackslash}X}

%\usemintedstyle{borland}

\usepackage[backend=biber]{biblatex}
\addbibresource{refs.bib}

% Title content
\title{Relatório - EP3}
\author{Piero Conti Kauffmann (8940810)}
\date{}

\begin{document}

\maketitle

% Introduction and Overview
\section{Modelos utilizados}

\subsection{Encoder-Decoder BiLSTM}

Pelas especificações da tarefa, o primeiro modelo proposto para o problema de geração automática de títulos  deve ser uma rede neural encoder-decoder com um encoder LSTM bidirecional ($\overrightarrow{g_{e}}$ e $\overleftarrow{g_{e}}$) acoplado a um mecanismo de atenção (Figura \ref{lstm_fig}). 

Na componente do decoder do modelo escolhi usar duas LSTMs unidirecionais em sequência. A primeira LSTM recebe o embedding do último token escrito no título e é inicializada com os estados finais da rede bidirecional concatenados, portanto possuí o dobro de \textit{hidden units} de $\overrightarrow{g_{e}}$ e $\overleftarrow{g_{e}}$. Além disso, é na primeira LSTM do decoder que é feito o cálculo dos vetores de contexto por meio do mecanismo de atenção do modelo, descrito adiante. Esses vetores são concatenados aos \textit{hidden states} da LSTM e são passados como entrada para a LSTM final, que finaliza a decodificação do próximo token da sequência.

\vspace{2em}

\begin{figure}[h]
\centering
\input{lstm_diag.tikz}
\caption{Diagrama do modelo encoder-decoder BiLSTM com camada de atenção escolhido. Os tokens especiais \texttt{[bos]} e \texttt{[eos]} delimitam respectivamente o início e fim das sequências de texto.}
\label{lstm_fig}
\end{figure}

O mecanismos de atenção adotado para o modelo foi o mecanismo de atenção global proposto por \textcite{luong2015effective} com escores de atenção obtidos a partir do produto interno dos \textit{hidden states}. Graças à existência da segunda LSTM do decoder, o modelo é também é capaz de incorporar a informação dos vetores de contexto dos tokens passados.


\subsection{BERT-CLS e BERT-MASK}

Neste trabalho iremos experimentar com duas variantes de soluções baseadas no BERT (representado na Figura \ref{bert_fig}) para geração de títulos de maneira autoregressiva. A primeira variante consiste em utilizar o campo de classificação (token \texttt{[CLS]}) de um modelo BERT pré-treinado para a língua portuguesa (\textcite{souza2020bertimbau}) para prever o próximo token do título em sequência.

A camada final associada ao token \texttt{[CLS]} do BERT é pré-treinada com a tarefa de \textit{Next Sentence Prediction} (NSP), que consiste em tentar adivinhar se a segunda sentença fornecida (separada pelo token \texttt{[SEP]}) vem depois da primeira sentença em um texto corrido. Para o problema em questão, podemos descartar a camada densa de classificação binária usada na tarefa de NSP e criar uma nova camada densa com a mesma dimensão final do vocabulário de saída, utilizando essa nova componente para prever o próximo token do título de maneira autoregressiva como se estivessemos resolvendo um problema de classificação supervisionada. Chamaremos esse modelo, descrito no enunciado do exercício, de BERT-CLS.

\begin{figure}[h]
	\centering
	\input{bert_diag.tikz}
	\caption{Diagrama ilustrativo do BERT para a task compartilhada de \textit{Masked Language Modelling} e \textit{Next Sentence Prediction}. }
	\label{bert_fig}
\end{figure}

Alternativamente, podemos tentar aproveitar a semelhança do objetivo deste trabalho com a tarefa de \textit{Masked Language Modelling} (MLM) do BERT pré-treinado, que tenta recuperar os tokens que são mascarados aleatóriamente nas sentenças passadas para o modelo. Apesar de parecer a melhor alternativa, existem algumas desvantagens aparentes: no objetivo de geração de texto deste trabalho, vamos sempre adicionar o token \texttt{[MASK]} no final da segunda sentença (título), que estará sempre seguido do token final \texttt{[SEP]}. Naturalmente, isto irá fazer o modelo original acreditar que o token mascarado é sempre um token próximo ao final de sentença (um caractere de ponto final, por exemplo) e irá prejudicar a performance do texto que iremos gerar de maneira iterativa. Ao fazermos o \textit{fine tuning} segundo esta abordagem, estamos grosseiramente tentando converter um modelo treinado com a tarefa de \textit{Masked Language Modelling} para um modelo causal de geração de títulos.

É difícil de prever de antemão se o BERT-MASK produzirá resultados melhores ou piores que o BERT-CLS (proposto originalmente pelo enunciado da tarefa), então, a título de curiosidade, decidi avaliar a performance destas duas abordagens neste trabalho.

\section{Treinamento}

\subsection{Encoder-Decoder BiLSTM}

O treinamento da encoder-decoder BiLSTM é feito utilizando a técnica \textit{teacher forcing} (\textcite{williams1989learning}), que passa a sequência correta inteira de tokens para o decoder que tenta prever o próximo token relativamente a cada item da sequência passada. Essa técnica na prática acelera a convergência de modelos sequenciais e usualmente também facilita a implementação computacional destes modelos.

Treinamos o modelo com \textit{early-stopping} em uma amostra de 72\% do conjunto de dados da base completa da B2W, validando o modelo ao final de cada época em um conjunto de validação que corresponde a 8\% dos dados. O treinamento é interrompido se o custo calculado no conjunto de validação não diminuir em 5 épocas consecutivas. Quando o treinamento é interrompido, o modelo com menor custo no conjunto de validação é salvo para a etapa de previsão.

Na Figura \ref{att_map}, é representado o mapa de atenção extraido do modelo para um exemplo do conjunto de validação. Verificamos que o modelo treinado foca a atenção em algumas palavras chaves úteis para gerar o título "Produto bom", como "não trava" e "Sem comentários".

\begin{figure}[h!]
	\scalebox{0.73}{\input{fig2.pgf}}
	\centering
	\caption{Mapa de atenção para uma avaliação do conjunto de validação com o título gerado pelo modelo recursivamente (eixo horizontal) e a avaliação submetida pelo cliente (eixo vertical).}
	\label{att_map}
\end{figure}

\subsection{BERT-CLS e BERT-MASK}

Seguindo o procedimento de preparação dos dados descrito no enunciado do exercício, utilizamos a mesma amostra de 80\% dos dados da base completa da B2W extraidos para o treinamento da LSTM. Após dividir os dados para a geração de texto token a token (conforme descrito no enunciado) o conjunto de treino final gerado possui cerca de 600 mil instâncias. Dada o elevado número de instâncias de treinamento e o alto nível de esforço computacional necessário no treinamento, os modelos são treinados durante apenas uma única época completa no conjunto de treinamento. Por sua vez, como a função de custo foi calculada apenas em dados que não foram vistos previamente pelo modelo, não foi necessário utilizar \textit{early-stopping} para previnir \textit{overfitting}.

O modelo BERT-CLS convergiu de maneira estável, sem precisar de um tamanho de \textit{batch} muito elevado, finalizando após cerca de 10h de treinamento no \textit{Google Colab}. O modelo BERT-MASK exigiu  um esforço maior de treinamento e teve convergência difícil, o que é justificável, visto que neste modelo alteramos a tarefa base de \textit{Masked Language Modelling} em que o modelo foi originalmente treinado.

Para garantir maior estabilidade no treinamento do BERT-MASK, foi necessário utilizar um tamanho de \textit{batch} maior, e consequentemente um número maior de iterações de \textit{gradient accumulation} para possibilitar o treinamento na infraestrutura disponível para o EP. O treinamento do modelo durou cerca de 20h na plataforma do \textit{Google Colab}.


As especificações principais de treinamento de ambos modelos podem ser consultadas na Tabela \ref{hyp} a seguir.

\begin{table}[h!]
	\centering
	\begin{tabular}{@{}lrrrr@{}}
		\toprule
		Modelo    & \multicolumn{1}{l}{lr} & \multicolumn{1}{l}{ \textit{batch}} & \multicolumn{1}{l}{GAcc} & \multicolumn{1}{l}{Total \textit{batch} / iteração} \\ \midrule
		BERT-CLS  & 5e-5                   & 10                                & 2                        & 20                                                 \\
		BERT-MASK & 5e-6                   & 12                                & 4                        & 48                                                 \\ \bottomrule
	\end{tabular}
	\caption{Hiperparâmetros de treinamento dos modelo BERT-CLS e BERT-MASK: taxa de aprendizado do otimizador (lr), tamanho do \textit{batch} (\textit{batch}), número de iterações de \textit{gradient accumulation} (GAcc) e o tamanho total do \textit{batch} por iteração.}
	\label{hyp}
\end{table}

%Para encomtrar as especificações exatas do treinamento dos modelos, consulte o arquivo \texttt{README.md}.



\section{Resultados experimentais}

Os modelos foram avaliados na amostra de testes (que representa 20\% do conjunto de dados completo da B2W) segundo as métricas:

\begin{itemize}
	\item Acurácia
	\item BLEU-1 (unigramas)
	\item BLEU-2 (unigramas e bigramas)
	\item BLEU-3 (unigramas, bigramas e trigramas)
	\item BLEU-4 (unigramas, bigramas, trigramas e quadrigramas)
	\item Medida-Namorada
	\item METEOR
\end{itemize}

Como os títulos gerados pelos modelos são usualmente curtos, o que pode impossibilitar a geração de n-gramas, usarei o suavizador proposto por \textcite{lin2004automatic} para calcular as precisões da métrica BLEU:

\begin{equation}
P_n = \frac{|\text{Modelo}_n \cap \text{Ref}_n| + 1}{|\text{Modelo}_n| + 1}, \quad n \in \{2, 3, 4\}
\end{equation}

\noindent onde $\text{Ref}_n$ e $\text{Modelo}_n$ são respectivamente os conjuntos dos n-gramas do texto de referência e do título gerado pelo modelo. Para um estudo comparativo entre diferentes técnicas de suavização, ver \textcite{chen2014systematic}.

A métrica Medida-Namorada foi obtida a partir da avaliação manual de uma avaliadora externa que não participou do desenvolvimento deste trabalho. A avaliação consistiu em dar uma nota de 0 à 10 para os títulos gerados pelos três modelos a partir de uma amostra de 200 avaliações do conjunto de testes. No final da avaliação, a nota foi normalizada para uma escala de 0 à 1. A planilha de preenchimento com os resultados de avaliação pode ser consultada  \href{https://docs.google.com/spreadsheets/d/1v9te15-LVNhdp3a1Iksk0YgP8a0gX4CPlsu6Jc0_5cI/edit?usp=sharing)}{neste link}.


Os resultados obtidos pelos três modelos são apresentados na Tabela \ref{table:results}.

\begin{table}[h!]
	\centering
	\begin{tabular}{@{}lrrrrrrr@{}}
		\toprule
		Modelo &
		\multicolumn{1}{l}{Acurácia} &
		\multicolumn{1}{l}{BLEU-1} &
		\multicolumn{1}{l}{BLEU-2} &
		\multicolumn{1}{l}{BLEU-3} &
		\multicolumn{1}{l}{BLEU-4} &
		\multicolumn{1}{l}{METEOR} &
		\multicolumn{1}{l}{M-Namorada} \\ \midrule
		BiLSTM    & 2,7\%          & 16,5\%          & 9,8\%           & 6,7\%          & 4,9\%          & 10,8\%          & 49,8\%          \\
		BERT-CLS  & 3,5\%          & 17,0\%          & 10,4\%          & 6,8\%          & 4,7\%          & 10,3\%          & 61,3\%          \\
		BERT-MASK & \textbf{4,9\%} & \textbf{17,6\%} & \textbf{11,1\%} & \textbf{7,6\%} & \textbf{5,4\%} & \textbf{12,5\%} & \textbf{65,5\%} \\ \bottomrule
	\end{tabular}
	\caption{Métricas avaliadas no conjunto de testes para os três modelos.}
	\label{table:results}
\end{table}

Verificamos que nas 7 métricas usadas para comparar os modelos, o modelo BERT-MASK foi o que obteve o melhor resultado, enquanto a BiLSTM apresentou em média o pior resultado dos três modelos. Comparando os resultados obtidos na avaliação humana entre os modelos BERT-CLS e BERT-MASK, podemos hipotetizar que mesmo com grandes alterações na tarefa original, o modelo BERT-MASK pareceu aproveitar melhor o modelo pré-treinado. Porém, para testar efetivamente essa hipótese seria necessário um tratamento estatístico mais adequado e um número maior de experimentos.

É possível verificar também que na métrica medida via avaliação humana (M-Namorada), o gap entre os modelos baseados no BERT e a BiLSTM é muito maior do que nas demais métricas. Uma análise mais detalhada desta métrica revela que o modelo BiLSTM parece produzir títulos muito simplistas e às vezes é incapaz de identificar corretamente a situação que o cliente descreve, alguns exemplos destas situações são apresentados na Tabela \ref{exemplos}.

\begin{table}
\centering

\begin{tabular}{p{4cm}llllp{3.5cm}l}
\toprule
\textit{Review} & BiLSTM & N & BERT-CSL & N & BERT-MASK & N \\ \midrule

\parbox{4cm}{Ótima bicicleta , minha sobrinha amou... chegou na data prevista ... vendedor excelente} & Excelente produto & 7  & Ótimo produdo & 7 & Ótima bicicleta & 10 \\

\\  \\

\parbox{4cm}{Estou aguardando a troca do produto, enviei pelo correio o produto com defeito e estou esperando a loja retornar com a resposta sobre a troca ou devolução do valor.} & Não recebi o produto & 0 & Produto com defeito & 7 & Produto com defeito & 7 \\

\\ \\

\parbox{4cm}{Não gostei do produto péssima qualidade. Soltou todas os encaixe de metal..Solicito a troca do produto ou devolução do dinheiro. ... os encaixe de metal soltaram e não mas na piscina e uma lona final e simples....fiquei decepciona com produto fora o atraso na entrega. ....péssimo aguardo solução} & Não recebi o produto & 0 & Péssimo produto & 7 & Não gostei do produto péssima qualidade. Soltou & 10 \\

\\  \\

\parbox{4cm}{Só imprime, nem cópia ela tira! Tão simples que poderia pelo menos vir com o cabo para o PC. Ela é branca, mas o fio de luz é preto. Impressora quebra galho, apenas compre como último recurso!} & Bom & 0 & PÉSSIMA & 6 & Impressora quebra galho & 9

\\ \bottomrule

\end{tabular}
\caption{Exemplos selecionados da avaliação humana (Medida-Namorada) dos títulos gerados pelos três modelos e suas respectivas notas (0-10). Foram escolhidos exemplos em que o modelo BiLSTM não é capaz de identificar corretamente a situação ou de produzir um título com bom nível de detalhamento.} 
\label{exemplos}
\end{table}

\printbibliography

\end{document}